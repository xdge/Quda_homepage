(function(e){function t(t){for(var i,o,r=t[0],l=t[1],c=t[2],u=0,p=[];u<r.length;u++)o=r[u],Object.prototype.hasOwnProperty.call(s,o)&&s[o]&&p.push(s[o][0]),s[o]=0;for(i in l)Object.prototype.hasOwnProperty.call(l,i)&&(e[i]=l[i]);d&&d(t);while(p.length)p.shift()();return n.push.apply(n,c||[]),a()}function a(){for(var e,t=0;t<n.length;t++){for(var a=n[t],i=!0,r=1;r<a.length;r++){var l=a[r];0!==s[l]&&(i=!1)}i&&(n.splice(t--,1),e=o(o.s=a[0]))}return e}var i={},s={app:0},n=[];function o(t){if(i[t])return i[t].exports;var a=i[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,o),a.l=!0,a.exports}o.m=e,o.c=i,o.d=function(e,t,a){o.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},o.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,t){if(1&t&&(e=o(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(o.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var i in e)o.d(a,i,function(t){return e[t]}.bind(null,i));return a},o.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return o.d(t,"a",t),t},o.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},o.p="/";var r=window["webpackJsonp"]=window["webpackJsonp"]||[],l=r.push.bind(r);r.push=t,r=r.slice();for(var c=0;c<r.length;c++)t(r[c]);var d=l;n.push([0,"chunk-vendors"]),a()})({0:function(e,t,a){e.exports=a("56d7")},"034f":function(e,t,a){"use strict";var i=a("85ec"),s=a.n(i);s.a},"4b33":function(e,t,a){},"56d7":function(e,t,a){"use strict";a.r(t);a("e260"),a("e6cf"),a("cca6"),a("a79d");var i=a("2b0e"),s=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{attrs:{id:"app"}},[a("HomePage")],1)},n=[],o=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{attrs:{id:"page-top"}},[a("nav",{staticClass:"navbar navbar-default navbar-fixed-top",attrs:{id:"mainNav",role:"navigation"}},[a("div",{staticClass:"container-fluid"},[a("div",{staticClass:"navbar-header"},[a("a",{staticClass:"navbar-brand page-scroll",attrs:{href:"#page-top"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("QUDA")])]),a("div",{attrs:{id:"bs-scrollspy"}},[a("ul",{staticClass:"nav navbar-nav navbar-right"},[a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"#details"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Six Dimensions")])]),a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"#overview"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Overview")])]),a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"#freenli"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Free-NLI")])]),a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"#team"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Team")])]),a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"https://freenli.github.io/quda/"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Download")])]),a("li",[a("a",{staticClass:"page-scroll",attrs:{href:"#contact"},on:{click:function(t){return e.smoothScroll(t)}}},[e._v("Contact")])])])])])]),a("header",[a("div",{staticClass:"header-content"},[a("div",{staticClass:"header-content-inner"},[a("h1",[e._v(e._s(e.title)+" ")]),a("hr"),a("h3",[e._v(e._s(e.subtitle))]),a("p",{staticClass:"text-faded"},[e._v(" "+e._s(e.projectDesc)+" "),a("br"),a("br"),e._v(" "+e._s(e.projectSubDesc)+" ")])])])]),a("section",{staticClass:"bg-primary",attrs:{id:"details"}},[a("div",{staticClass:"container"},[a("div",{staticClass:"row"},[a("div",{staticClass:"col-lg-8 col-lg-offset-2 text-center"},[a("h2",{staticClass:"section-heading"},[e._v("Six Dimensions")]),a("hr",{staticClass:"light"}),e._l(e.aboutDetails,(function(t){return a("div",{key:t[0][0],staticClass:"row six-dimensions"},e._l(t,(function(t){return a("div",{key:t[0],staticClass:"col-md-6 col-sm-6 col-xs-12"},[a("div",{staticClass:"media",staticStyle:{"padding-bottom":"10px"}},[a("div",{staticClass:"media-left"},[a("img",{staticClass:"media-object img-rounded",attrs:{width:"100",src:t[2],alt:t[2]}})]),a("div",{staticClass:"media-body",staticStyle:{"vertical-align":"middle"}},[a("p",{staticClass:"dimensions-text"},[e._v(e._s(t[0]))]),a("p",{staticClass:"text-faded dimensions-text"},[e._v(e._s(t[1]))])])])])})),0)}))],2)])])]),e._m(0),a("section",{staticClass:"bg-dark",attrs:{id:"contact"}},[a("div",{staticClass:"container"},[a("div",{staticClass:"row"},[a("div",{staticClass:"col-md-12 text-center"},[a("h2",{staticClass:"section-heading"},[e._v(e._s(e.contactTitle))]),a("hr",{staticClass:"primary"}),a("p",[e._v(e._s(e.contactDesc))])]),e._m(1)])])])])},r=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("section",{attrs:{id:"overview"}},[a("div",{staticClass:"container"},[a("div",{staticClass:"row",staticStyle:{"margin-top":"20px"}},[a("div",{staticClass:"col-lg-8 col-lg-offset-2 text-center"},[a("h2",{staticClass:"section-heading"},[e._v("Corpus Overview")]),a("hr",{staticClass:"primary"}),a("h3",[a("i",{staticClass:"fa fa-tasks"}),e._v(" The analysis of 10 tasks")]),a("img",{staticClass:"img-rounded",attrs:{width:"100%",src:"/img/overview.png"}})])]),a("div",{staticClass:"row",staticStyle:{"margin-top":"20px"}},[a("div",{staticClass:"col-lg-8 col-lg-offset-2 text-center"},[a("h3",[a("i",{staticClass:"fa fa-table"}),e._v(" Samples")]),a("img",{staticClass:"img-rounded",attrs:{width:"100%",src:"/img/infographic.png"}})])])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-md-12 text-center"},[a("i",{staticClass:"fa fa-envelope-o fa-3x wow bounceIn",attrs:{"data-wow-delay":".1s"}}),a("p",[a("a",{attrs:{href:"mailto:xdge@zju.edu"}},[e._v("xdge@zju.edu")])])])}],l=(a("4160"),a("b0c0"),a("159b"),a("1157")),c=a.n(l),d=(a("1b58"),a("ab8b"),a("3e48"),a("4989"),a("4b33"),a("1f54"),{name:"HomePage",data:function(){return{title:"QUDA",subtitle:"Natural Language Queries for Visual Data Analytics",projectDesc:"Visualization-oriented natural language interfaces (V-NLIs) have been explored and developed in recent years. One challenge faced by V-NLIs is in the formation of effective design decisions that usually requires a deep understanding of user queries. We present a new dataset, called Quda,  to help V-NLIs categorize queries into analytic tasks by training and benchmarking cutting-edge NLP techniques. Our corpus consists of a number of queries annotated with an analytic task and features high-quality, large-volume queries designed in the context of V-NLIs.",projectSubDesc:"Quda contains 14,035 diverse user queries annotated with 10 low-level analytic tasks that assist in the deployment of state-of-the-art techniques for parsing complex human language.",aboutDetails:[[["Abstraction: Concrete","Focus on the queries at the low abstraction level","/img/icons/taxonomy.png"],["Composition: Low","10s of eye-tracking lab participants","/img/icons/10s-of-people.png"]],[["Perspective: Objectives","100s of labeled visualizations","/img/icons/100s-labeled-viz.png"],["Type of Data: Table","100s of memorability scores","/img/icons/100s-mem-scores.png"]],[["Type of Tasks: 10 Low-level Tasks","100s of participants on Amazon’s Mechanical Turk","/img/icons/100s-of-people.png"],["Context Dependency: Independent",'1000s of visualizations "in-the-wild"',"/img/icons/1000s-visualization.png"]]],eyeTitle:"Eye-movement Data",eyeDesc:"We have eye-movement data for a total of 393 visualizations and 33 viewers, with an average of 16 viewers per visualization. Each viewer looked at each visualization for 10 seconds, generating an average of 37 fixation points. This is a total of about 600 fixation points per visualization across all viewers. We store the (x,y) location of each fixation on a visualization, the time-point when the fixation occurred during the viewing period, and the duration (in ms) of each fixation. We provide tools for visualizing the fixation sequences, fixation durations, and fixation heatmaps on top of visualizations.",bookChapters:[{title:"Eye Fixation Metrics for Large Scale Evaluation and Comparison of Information Visualizations",link:"http://link.springer.com/chapter/10.1007/978-3-319-47024-5_14",bibtex:"http://citation-needed.services.springer.com/v2/references/10.1007/978-3-319-47024-5_14?format=bibtex&flavour=citation",authors:"Bylinskii, Z., Borkin, M. A., Kim, N. W., Pfister, H., and Oliva, A.",source:"In Burch, M., Chuang, L., Fisher, B., Schmidt, A., Weiskopf, D. (Eds.), Eye Tracking and Visualization: Foundations, Techniques, and Applications (pp. 235-255). Springer International Publishing"}],journalPapers:[{title:"BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention",link:"https://vcg.seas.harvard.edu/publications/bubbleview-an-alternative-to-eye-tracking-for-crowdsourcing-image-importance/paper",site:"https://namwkim.github.io/bubbleview/",bibtex:"https://vcg.seas.harvard.edu/publications/bubbleview-an-alternative-to-eye-tracking-for-crowdsourcing-image-importance.bib",supplement:"https://vcg.seas.harvard.edu/publications/bubbleview-an-alternative-to-eye-tracking-for-crowdsourcing-image-importance/supplementary-material",authors:"Kim, N.W.<sup>*</sup>, Bylinskii, Z.<sup>*</sup>, Borkin, M., Gajos, K.Z., Oliva, A., Durand F., & Pfister, H.",source:"ACM Transactions on Computer-Human Interaction, 2017 (to appear)"},{title:"Beyond Memorability: Visualization Recognition and Recall",link:"http://vcg.seas.harvard.edu/files/pfister/files/infovis_submission251-camera.pdf",supplement:"http://vcg.seas.harvard.edu/files/pfister/files/infovis_submission251-supplementalmaterial-camera.pdf",video:"http://vcg.seas.harvard.edu/files/pfister/files/infovis-251_teaser.mp4",slides:"http://vcg.seas.harvard.edu/files/pfister/files/infovis-2015_final.pdf",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/534661",authors:"Borkin, M.<sup>*</sup>,  Bylinskii, Z.<sup>*</sup>, Kim, N.W., Bainbridge C.M., Yeh, C.S., Borkin, D., Pfister, H., & Oliva, A.",source:"IEEE Transactions on Visualization and Computer Graphics (Proceedings of InfoVis 2015)"},{title:"What Makes a Visualization Memorable?",link:"http://vcg.seas.harvard.edu/files/pfister/files/infovis_borkin-128-camera_ready_0.pdf",supplement:"http://vcg.seas.harvard.edu/files/pfister/files/supplemental-infovis128.pdf",video:"http://vcg.seas.harvard.edu/files/pfister/files/experiment-screengrab.mp4",slides:"http://vcg.seas.harvard.edu/files/pfister/files/infovis2013_borkin-vizmem.pdf",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/83476",authors:"Borkin, M., Vo, A., Bylinskii, Z., Isola, P., Sunkavalli, S., Oliva, A., & Pfister, H.",source:"IEEE Transactions on Visualization and Computer Graphics (Proceedings of InfoVis 2013)"}],otherPapers:[{title:"Eye Fixation Metrics for Large Scale Analysis of Information Visualizations",link:"http://web.mit.edu/zoya/www/Bylinskii_eyefixations_small.pdf",slides:"http://web.mit.edu/zoya/www/ETVIS_red.pdf",authors:"Bylinskii, Z., & Borkin, M.",source:"First Workshop on Eyetracking and Visualizations (ETVIS 2015) in conjunction with IEEE VIS 2015"},{title:"A Crowdsourced Alternative to Eye-tracking for Visualization Understanding",link:"https://vcg.seas.harvard.edu/publications/a-crowdsourced-alternative-to-eye-tracking-for-visualization-understanding/paper",slides:"https://vcg.seas.harvard.edu/publications/a-crowdsourced-alternative-to-eye-tracking-for-visualization-understanding/poster",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/371751",authors:"Kim, N.W., Bylinskii, Z., Borkin, M., Oliva, A., Gajos, K.Z., & Pfister, H.",site:"https://namwkim.github.io/bubbleview/",source:"Proceedings of the ACM Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '15)"}],techReports:[],members:[{name:"Michelle A. Borkin",url:"http://people.seas.harvard.edu/~borkin/",photo:"michelle.jpg",affiliation:"Assistant Professor, Northeastern CCIS"},{name:"Zoya Bylinskii",url:"http://web.mit.edu/zoya/www/",photo:"zoya.jpg",affiliation:"PhD Candidate, MIT EECS"},{name:"Krzysztof Z. Gajos",url:"http://www.eecs.harvard.edu/~kgajos/",photo:"gajos.jpg",affiliation:"Assosiate Professor, Harvard SEAS"},{name:"Nam Wook Kim",url:"http://namwkim.org",photo:"nam.png",affiliation:"PhD Candidate, Harvard SEAS"},{name:"Aude Oliva",url:"http://cvcl.mit.edu/audeoliva.html",photo:"aude.jpg",affiliation:"Principal Research Scientist, MIT CSAIL"},{name:"Hanspeter Pfister",url:"http://vcg.seas.harvard.edu/people/hanspeter-pfister",photo:"hp.jpg",affiliation:"Professor, Harvard SEAS"}],datasetTitle:"Dataset Download",license:"By checking this box, you agree to the following license agreement: Access to, and use of, the images, and annotations in this dataset are for research and educational uses only. No commercial use, reproduction or distribution of the images, or any modifications thereof, is permitted. ",datasets:[{name:"all5k",size:"(~2.42G)",desc:"This data contains 5,814 single- and multi-panel visualizations scraped from the web from seven different online sources making up a total of four different source categories (government and world organizations, news media, infographics, and scientific publications). We provide the original visualizations, original URLs, source and category labels, as well as whether each visualization is single or multi-panel. This data is described in “What makes a visualization memorable?” (InfoVis 2013).",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/83476"},{name:"single2k",size:"(~625M)",desc:"This data contains a subset of the visualizations from all5k, limited to only single-panel, stand-alone visualizations (a total of 2,068 visualizations). We provide the original URLs, source and category labels, and visualization type. The taxonomy used to classify the visualization type is described in “What makes a visualization memorable?” (InfoVis 2013).",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/83476"},{name:"targets410",size:"(~140M)",desc:"This data contains taxonomic labels and attributes for 410 visualizations. These include the source, category, and type of each visualization, as well as the following attributes: data-ink ratio, number of distinctive colors, black & white, visual density, human recognizable object (HRO), and human depiction. We also provide the transcribed title for each visualization and where the title was located on the visualization. From the Amazon Mechanical Turk (AMT) Experiments, we provide the number of hits, misses, false alarms, and correct rejections per image, which can be converted into the desired memorability scores (HR, FAR, dprime, etc.)",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/83476"},{name:"targets393",size:"(~160M)",desc:"This data contains taxonomic labels and attributes for 393 visualizations. These include the source, category, and type of each visualization, as well as the following attributes: data-ink ratio, number of distinctive colors, black & white, visual density, human recognizable object (HRO), and human depiction. We also provide the transcribed title for each visualization and where the title was located on the visualization, as well as whether the visualization contained data or message redundancy. From Borkin et al. 2013 we include at-a-glance memorability scores (after 1 second of viewing) and from Borkin, Bylinskii et al. 2015 we include prolonged memorability scores (after 10 seconds of viewing). As described in “Beyond Memorability: Visualization Recognition and Recall“ (InfoVis 2015), we provide participant's eye movements and textual descriptions.",bibtex:"http://vcg.seas.harvard.edu/publications/export/bibtex/534661"}],name:"",email:"",inst:"",instAddr:"",ivgrName:"",ivgrEmail:"",othrEmail:"",rqstResn:"",contactTitle:"Contact us!",contactDesc:"Questions? Comments?",acknowledgement:"This work has been supported in part by the National Science Foundation (NSF) under grant 1016862, MIT Big Data Initiative at CSAIL, Google, and Xerox awards to Aude Oliva. This work has also been made possible through support from the Department of Defense through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program, the NSF Graduate Research Fellowship Program, the Natural Sciences and Engineering Research Council of Canada Postgraduate Doctoral Scholarship (NSERC PGS-D), and the Kwanjeong Educational Foundation."}},methods:{smoothScroll:function(e){var t=c()(e.target);c()("html, body").stop().animate({scrollTop:c()(t.attr("href")).offset().top-50},1250),e.preventDefault()},validateEmail:function(e){var t=/^([\w-]+(?:\.[\w-]+)*)@((?:[\w-]+\.)*\w[\w-]{0,66})\.([a-z]{2,6}(?:\.[a-z]{2})?)$/i;return t.test(e)},downloadDataset:function(){if(0==c()("#licensechk").prop("checked"))return c()("#license-alert").show(),void setTimeout((function(){c()("#license-alert").hide(400)}),2e3);if(console.log(this.name+","+this.email+","+this.inst+","+this.instAddr+","+this.ivgrName+","+this.ivgrEmail+","+this.othrEmail+","+this.rqstResn),""==this.name||""==this.email||!this.validateEmail(this.email))return console.log("not enough information"),c()("#request-alert").show(),void setTimeout((function(){c()("#request-alert").hide(400)}),2e3);var e=[];return this.datasets.forEach((function(t){c()("#"+t.name).prop("checked")&&("all5k"==t.name?(e.push("news"),e.push("science"),e.push("government"),e.push("vis1"),e.push("vis2"),e.push("vis3")):e.push(t.name))})),0==e.length?(c()("#data-alert").show(),void setTimeout((function(){c()("#data-alert").hide(400)}),2e3)):void 0}},mounted:function(){c()("#mainNav").affix({offset:{top:100}})}}),u=d,p=(a("a3bb"),a("2877")),h=Object(p["a"])(u,o,r,!1,null,null,null),f=h.exports,v={name:"App",components:{HomePage:f}},m=v,g=(a("034f"),Object(p["a"])(m,s,n,!1,null,null,null)),b=g.exports;i["a"].config.productionTip=!1,new i["a"]({render:function(e){return e(b)}}).$mount("#app")},"85ec":function(e,t,a){},"96b4":function(e,t,a){},a3bb:function(e,t,a){"use strict";var i=a("96b4"),s=a.n(i);s.a}});
//# sourceMappingURL=app.59b26dd9.js.map